{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d06df7-e0f4-42ce-961c-b00c38db3542",
   "metadata": {},
   "source": [
    "# OrchestrAit\n",
    "## Mortgage Packet Sorting Made Easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc29f7-04e6-4f77-bb75-3242d4179a84",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This notebook serves as a proof of concept showing how scripting coupled with a trained ML model can automate the process of loan packet sorting. The scripts take care of folder creation and routing while the model reads data to determine document categories. Keep in mind your company's current workflow as you see how files and folders are created. The follwing questions will need to be answered when implementing OrchestrAit into your workflow:\n",
    "\n",
    "1. What is the naming convention of your current loan packets?\n",
    "2. Where are all your unprocessed loan packets currently stored? And in what format do they come in as?\n",
    "3. What is the current file structure of your processed loan packets? What format are they stored in?\n",
    "4. What programs need access to these processed loan packets? (E.g. Salesforce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc4079-cfd9-4b21-bc31-06d44e16d2cc",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Below we initialize some necessary python libraries and import the trained model to action on our sample files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a52c1c33-ca93-495a-9700-de9689fdbdb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Tim\n",
      "[nltk_data]     Williams\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import shutil\n",
    "import pathlib\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import cv2\n",
    "import nltk\n",
    "import shutil\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "nltk.download('punkt')  # Download the required resource\n",
    "pytesseract.pytesseract.tesseract_cmd =r\"C:\\Program Files\\Tesseract-OCR\\tesseract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36b7a5ba-1800-449a-a785-facddd52ef9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model('mortgage_doc_identifier_v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56884de8-f66e-4052-b6b4-fb9570fbf32e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (Custom>TFBertMainLayer)  {'last_hidden_state  109482240   ['input_ids[0][0]',              \n",
      "                                ': (None, 512, 768)               'attention_mask[0][0]']         \n",
      "                                , 'pooler_output':                                                \n",
      "                                (None, 768)}                                                      \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1024)         787456      ['bert[0][1]']                   \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 6)            6150        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,275,846\n",
      "Trainable params: 110,275,846\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27303ef-8bdb-4191-997c-ff7a486fc77e",
   "metadata": {},
   "source": [
    "To begin, we also define some loan document types. We can define as many as we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1452048f-67f9-4441-b20a-08d29bfef497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_type_df=pd.DataFrame({'doc_type': ['Foreclosure', 'General','Mortgage','Note','Origination','Title']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae368e03-0f59-48cf-b07c-4290f06d0a40",
   "metadata": {},
   "source": [
    "### Support Functions\n",
    "Next we define a few functions which will automatically create the folder structre we want for each loan packet. We also split each loan packet into its individual pages to prep for analyzing and routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45819c0b-5841-4700-9b7b-3bedda3fe17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subdir_maker(full_file_path, final_output_path):\n",
    "\n",
    "    # Extract the file name from the path\n",
    "    file_name = os.path.basename(full_file_path)\n",
    "\n",
    "    # Remove the file extension (if any)\n",
    "    file_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "    # Create a folder with the file name\n",
    "    folder_path = os.path.join(final_output_path, file_name)\n",
    "    os.makedirs(folder_path)\n",
    "    \n",
    "    \n",
    "    # create a list of these folders and iterate through the list to create new subdirs\n",
    "    folder_list=doc_type_df.doc_type.tolist()\n",
    "    for folder in folder_list:\n",
    "        new_subfolder_path = os.path.join(folder_path, folder)\n",
    "        os.makedirs(new_subfolder_path)\n",
    "    \n",
    "    # return the root folder path so that the split_pdf function can put the files that need to be sorted here.\n",
    "    return folder_path, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64074ca4-5563-46aa-a593-cb646de11682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf_template(input_path, final_output_path):\n",
    "    file_list = os.listdir(input_path)\n",
    "    doc_list=[]\n",
    "    # Step 3: Iterate over each file in the list\n",
    "    for file_name in file_list:\n",
    "        \n",
    "        file_name=file_name.replace(' ','_')\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(input_path, file_name)\n",
    "        \n",
    "# ********************** UPDATE FUNCTION IF FILE ALREADY EXISTS!!! ************************************** #    \n",
    "\n",
    "        new_folder, name = subdir_maker(file_path, final_output_path)\n",
    "\n",
    "        with open(file_path, 'rb') as file:\n",
    "            read_pdf = PyPDF2.PdfReader(file)\n",
    "\n",
    "            for page_number, page in enumerate(read_pdf.pages, start=1):\n",
    "                pdf_writer = PyPDF2.PdfWriter()\n",
    "                pdf_writer.add_page(page)\n",
    "\n",
    "                output_path = f\"{name}_{page_number}.pdf\"\n",
    "                with open(output_path, 'wb') as output_file:\n",
    "                    pdf_writer.write(output_file)\n",
    "                shutil.move(name+\"_\"+str(page_number)+\".pdf\", new_folder)\n",
    "                doc_list += [(new_folder,output_path, pdf_reader(pathlib.Path(new_folder,output_path)))]\n",
    "                \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa2b891-4757-4b40-87f2-62279ab508dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_reader(file):\n",
    "    read=convert_from_path(file, dpi=400)\n",
    "    for i in read:\n",
    "        image=cv2.cvtColor(np.array(i), cv2.COLOR_RGB2BGR)\n",
    "        # Grayscale, Gaussian blur, Otsu's threshold\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "        thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "        # Morph open to remove noise and invert image\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "        opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "        invert = 255 - opening\n",
    "        text=pytesseract.image_to_string(invert,lang='eng', config='--psm 6')\n",
    "    text=text.replace('\\n',' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3c16fd-37ee-4c28-b1ba-e54c6bb97849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def doc_to_dataframe(source_path, output_path):\n",
    "    doc_list=split_pdf_template(source_path, output_path)\n",
    "    doc_list_df=pd.DataFrame(doc_list, columns=['doc_path', 'doc_name','doc_content'])\n",
    "    doc_list_df=doc_list_df.dropna()\n",
    "    return doc_list_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cac3279-2607-40d3-8491-c52706acffff",
   "metadata": {},
   "source": [
    "### Execute our Support Functions for New Files and Folders\n",
    "\n",
    "Enter the location of the original files and the location of where the processed files should go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "540a7ab4-11d6-4512-b9c8-12c376a9cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the folder location of your source packet files C:\\Users\\Tim Williams\\Desktop\\python_playground\\unfiltered_forms\\actual\n",
      "Enter the folder location where you want to store your processed packet files C:\\Users\\Tim Williams\\Desktop\\python_playground\\unfiltered_forms\\final\n"
     ]
    }
   ],
   "source": [
    "sample_file = input('Enter the folder location of your source packet files')\n",
    "output_path = input('Enter the folder location where you want to store your processed packet files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448456b-0e31-4c55-94ab-e610f36a36b3",
   "metadata": {},
   "source": [
    "We segment text into set number of word sequences so that our model can ingest and analyze large documents (10K + words no problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5eeeff1-2fee-4e65-b06f-155942908671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def segment_text(text, max_length):\n",
    "    word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+') # tokenize words\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    segments = []\n",
    "    current_segment = []\n",
    "    for token in tokens:\n",
    "        current_segment.append(token)\n",
    "        if len(current_segment) == max_length:\n",
    "            segments.append(current_segment)\n",
    "            current_segment = []\n",
    "    # If there are remaining tokens at the end, add them as the last segment\n",
    "    if current_segment:\n",
    "        segments.append(current_segment)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42538617-dbfa-4ab1-a7c7-7557c11551ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame to store segmented text\n",
    "def word_segmenter(df, seg_size):\n",
    "    # Set the sequence length\n",
    "    max_sequence_length = seg_size\n",
    "    # Initialize an empty DataFrame to store the segmented text\n",
    "    doc_segments_df = pd.DataFrame(columns=['doc_path', 'doc_name', 'doc_content', 'doc_segments'])\n",
    "\n",
    "    # Iterate over each row in the doc_df DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        segments = segment_text(row['doc_content'], max_sequence_length)\n",
    "\n",
    "        # Create a new DataFrame for the segments of the current document\n",
    "        single_doc_segment_df = pd.DataFrame({'doc_path': row['doc_path'],\n",
    "                                              'doc_name': row['doc_name'],\n",
    "                                              'doc_content': row['doc_content'],\n",
    "                                              'doc_segments': [' '.join(segment) for segment in segments]})\n",
    "\n",
    "        # Append the segmented text of the current document to the doc_segments_df DataFrame\n",
    "        doc_segments_df = pd.concat([doc_segments_df, single_doc_segment_df], ignore_index=True)\n",
    "    return doc_segments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4191d51c-f153-45db-b038-c3c8876ec0da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_data(text):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    tokens=tokenizer.encode_plus(text,max_length=512,\n",
    "                                 truncation=True, padding='max_length',\n",
    "                                 add_special_tokens=True, return_token_type_ids=False,\n",
    "                                 return_tensors='tf')\n",
    "    return {\n",
    "        'input_ids': tf.cast(tokens['input_ids'], tf.float64),\n",
    "        'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cedd64f5-9555-46e8-be59-bf3fbe308e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_sort_data(df, seg_size):\n",
    "    doc_segments_df=word_segmenter(df, seg_size)\n",
    "    doc_segments_df['tokenized_segments']=doc_segments_df['doc_segments'].apply(lambda x: prep_data(x))\n",
    "    doc_segments_df['probability']=[model.predict(i)[0] for i in doc_segments_df.tokenized_segments]\n",
    "    agg_probs_df=pd.DataFrame(doc_segments_df.groupby(['doc_path','doc_name'])['probability'].sum()).reset_index()\n",
    "    agg_probs_df['doc_type_no']=agg_probs_df.probability.apply(lambda x: np.argmax(x))\n",
    "    agg_probs_df=agg_probs_df.merge(doc_type_df, left_on='doc_type_no', right_on=doc_type_df.index)\n",
    "    for index, row in agg_probs_df.iterrows():\n",
    "        shutil.move(os.path.join(row.doc_path,row.doc_name), os.path.join(row.doc_path,row.doc_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c94a907-0f94-4958-b1cd-952449edbab3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 88 entries, 0 to 87\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_path     88 non-null     object\n",
      " 1   doc_name     88 non-null     object\n",
      " 2   doc_content  88 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_path</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>doc_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Tim Williams\\Desktop\\python_playgroun...</td>\n",
       "      <td>COLFIL-7-18-2016-Loan_7000087874-102130061_1.pdf</td>\n",
       "      <td>wo ) oy : eo _ 4 : 3 ADJUSTABLE RATE NOTE é ; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Tim Williams\\Desktop\\python_playgroun...</td>\n",
       "      <td>COLFIL-7-18-2016-Loan_7000087874-102130061_2.pdf</td>\n",
       "      <td>/ ° : ON . ; &lt; . Co | D 4, INTEREST RATE AND M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Tim Williams\\Desktop\\python_playgroun...</td>\n",
       "      <td>COLFIL-7-18-2016-Loan_7000087874-102130061_3.pdf</td>\n",
       "      <td>” SO wy]  7. BORROWER'S FAILURE TO PAY AS REQU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Tim Williams\\Desktop\\python_playgroun...</td>\n",
       "      <td>COLFIL-7-18-2016-Loan_7000087874-102130061_4.pdf</td>\n",
       "      <td>: _ ‘ , - A . &amp; . Transfer of the Property or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Tim Williams\\Desktop\\python_playgroun...</td>\n",
       "      <td>COLFIL-7-18-2016-Loan_7000087874-102130061_5.pdf</td>\n",
       "      <td>casoc . wih * . i . Finance America, LLC 16802...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C:\\Users\\Tim Williams\\Desktop\\python_playgroun...</td>\n",
       "      <td>COLFIL-7-18-2016-Loan_7000087874-102130061_6.pdf</td>\n",
       "      <td>. . oe |  Loan number: 9800078868  Borrower: R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C:\\Users\\Tim Williams\\Desktop\\python_playgroun...</td>\n",
       "      <td>COLFIL-7-18-2016-Loan_7000087874-102130061_7.pdf</td>\n",
       "      <td>a, | cg Note Allonge Borrower:JOHN R. KAISER A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C:\\Users\\Tim Williams\\Desktop\\python_playgroun...</td>\n",
       "      <td>COLFIL-7-18-2016-Loan_7000087874-102130061_8.pdf</td>\n",
       "      <td>(pH4079 - a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C:\\Users\\Tim Williams\\Desktop\\python_playgroun...</td>\n",
       "      <td>COLFIL-7-18-2016-Loan_7000087874-102130061_9.pdf</td>\n",
       "      <td>) Borrower:JOHN R. KAISER AND ROSEMARIE T. KAI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C:\\Users\\Tim Williams\\Desktop\\python_playgroun...</td>\n",
       "      <td>COLFIL-7-18-2016-Loan_7000087874-102130061_10.pdf</td>\n",
       "      <td>[24047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_path  \\\n",
       "0  C:\\Users\\Tim Williams\\Desktop\\python_playgroun...   \n",
       "1  C:\\Users\\Tim Williams\\Desktop\\python_playgroun...   \n",
       "2  C:\\Users\\Tim Williams\\Desktop\\python_playgroun...   \n",
       "3  C:\\Users\\Tim Williams\\Desktop\\python_playgroun...   \n",
       "4  C:\\Users\\Tim Williams\\Desktop\\python_playgroun...   \n",
       "5  C:\\Users\\Tim Williams\\Desktop\\python_playgroun...   \n",
       "6  C:\\Users\\Tim Williams\\Desktop\\python_playgroun...   \n",
       "7  C:\\Users\\Tim Williams\\Desktop\\python_playgroun...   \n",
       "8  C:\\Users\\Tim Williams\\Desktop\\python_playgroun...   \n",
       "9  C:\\Users\\Tim Williams\\Desktop\\python_playgroun...   \n",
       "\n",
       "                                            doc_name  \\\n",
       "0   COLFIL-7-18-2016-Loan_7000087874-102130061_1.pdf   \n",
       "1   COLFIL-7-18-2016-Loan_7000087874-102130061_2.pdf   \n",
       "2   COLFIL-7-18-2016-Loan_7000087874-102130061_3.pdf   \n",
       "3   COLFIL-7-18-2016-Loan_7000087874-102130061_4.pdf   \n",
       "4   COLFIL-7-18-2016-Loan_7000087874-102130061_5.pdf   \n",
       "5   COLFIL-7-18-2016-Loan_7000087874-102130061_6.pdf   \n",
       "6   COLFIL-7-18-2016-Loan_7000087874-102130061_7.pdf   \n",
       "7   COLFIL-7-18-2016-Loan_7000087874-102130061_8.pdf   \n",
       "8   COLFIL-7-18-2016-Loan_7000087874-102130061_9.pdf   \n",
       "9  COLFIL-7-18-2016-Loan_7000087874-102130061_10.pdf   \n",
       "\n",
       "                                         doc_content  \n",
       "0  wo ) oy : eo _ 4 : 3 ADJUSTABLE RATE NOTE é ; ...  \n",
       "1  / ° : ON . ; < . Co | D 4, INTEREST RATE AND M...  \n",
       "2  ” SO wy]  7. BORROWER'S FAILURE TO PAY AS REQU...  \n",
       "3  : _ ‘ , - A . & . Transfer of the Property or ...  \n",
       "4  casoc . wih * . i . Finance America, LLC 16802...  \n",
       "5  . . oe |  Loan number: 9800078868  Borrower: R...  \n",
       "6  a, | cg Note Allonge Borrower:JOHN R. KAISER A...  \n",
       "7                                       (pH4079 - a   \n",
       "8  ) Borrower:JOHN R. KAISER AND ROSEMARIE T. KAI...  \n",
       "9                                            [24047   "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df = doc_to_dataframe(sample_file, output_path)\n",
    "doc_df.info() # Quick sanity check shows how many documents we have and if there are any empty rows in our dataframe\n",
    "doc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b68b6d53-ca37-4480-becd-7f8760bd6804",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wo ) oy : eo _ 4 : 3 ADJUSTABLE RATE NOTE é ; - ° (LIBOR Index - Rate Caps) . : . MIN 100052300401777767 THIS NOTE CONTAINS PROVISIONS ALLOWING FOR CHANGES IN MY INTEREST RATE AND MY MONTHLY PAYMENT. THIS NOTE LIMITS THE AMOUNT MY INTEREST RATE CAN CHANGE AT ANY ONE TIME AND THE MAXIMUM RATE I MUST PAY. . 04/24/04 IRVINE: CA sO, [Date] [City] {State] 2 FRANKLIN PL, MASSAPEQUA, NY 11758-7015 | [Property Address] ~ 1. BORROWER’S PROMISE TO PAY ; In return for a loan that I have received, [ promiseto payU.S.$ 415,000.00 (this amount is called “Principal\"), plus interest, to the order of the Lender. The Lender is Finance America, LLC ; . I will make all payments under this Note in the form of cash, check or money order. I understand that the Lender may transfer this Note. The Lender or anyone who takes this Note by transfer and who is entitled to receive payments under this Note is called the \"Note Holder.” 2. INTEREST | Interest will be charged on unpaid principal until the full amount of Principal has been paid. I will pay interest at a yearly rate of 6.340 %. The interest rate I will pay may change in accordance with Section 4 of this Note. The interest rate required by this Section 2 and Section 4 of this Note is the rate I will pay both before and after any default described in Section 7(B) of this Note. 3. PAYMENTS . ) (A) Time and Place of Payments ) I will pay principal and interest by making a payment every month. I will make my monthly payments on the first day of each month beginning on JUNE O01, 2004 . I will make these payments every month until J have paid all of the principal and interest and any other charges described below that I may owe under this Note. Each monthly payment will be applied as of its scheduled due date and will be applied to interest before Principal. If, on MAY O01, 2034 , I still owe amounts under this Note, I will pay those amounts in full on that date, which is called the \"Maturity Date.\" , I will make my monthly payments at OCWEN Federal Bank, FSB : P.O. Box 514577, Los Angeles, CA 90051-4577 or at a-different place if required by the Note Holder. (B) Amount of My Initial Monthly Payments ; Each of my initial monthly payments will be in the amountof U.\\'S.$ 2,579.57 . This amount may change. (C) Monthly Payment Changes . . Changesin my monthly payment will reflect changes in the unpaid principal of my loan and in the interest rate that J must pay. The Note Holder will determmemy new interest rate and the changed amount of my monthly payment in accordancewith Section 4 of this Note. | MULTISTATE ADJUSTABLE RATE NOTE - LIBOR INDEX - Single Family - Freddie Mac UNIFORM INSTRUMENT ) Form 3590 1/01 CD21 5N (0005).01 VMP MORTGAGE FORMS - (800)521-7291 : Page 1 of 4 Initials. fare MDAC \\\\ \" , LOAN ID: 0040177776 ~ '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df.doc_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9f0c404-f8db-4cef-bcab-8e26c204e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n"
     ]
    }
   ],
   "source": [
    "ml_sort_data(doc_df, 512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
